{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vietnamese_POS_Tagger.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvhzVmGjvB6ZqR4IOaRJdR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nthanhkhang/vietnamese-pos-tagging/blob/main/Vietnamese_POS_Tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyePADQUsrNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fba7aee0-318a-4d4a-a3c9-208420527d8f"
      },
      "source": [
        "!pip install pyvi\r\n",
        "import string\r\n",
        "import numpy\r\n",
        "import sklearn.svm as svm\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "import io\r\n",
        "from collections import defaultdict\r\n",
        "import random\r\n",
        "from pyvi import ViTokenizer, ViPosTagger"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/e1/0e5bc6b5e3327b9385d6e0f1b0a7c0404f28b74eb6db59a778515b30fd9c/pyvi-0.1-py2.py3-none-any.whl (8.5MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.22.2.post1)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.0.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.7 pyvi-0.1 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PguXY_QMKw4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7166e48-27be-4360-b839-a03daa582c92"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_train.txt\r\n",
        "!wget https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-03 14:40:22--  https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1664500 (1.6M) [text/plain]\n",
            "Saving to: ‘vi_train.txt’\n",
            "\n",
            "vi_train.txt        100%[===================>]   1.59M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-03-03 14:40:22 (40.0 MB/s) - ‘vi_train.txt’ saved [1664500/1664500]\n",
            "\n",
            "--2021-03-03 14:40:22--  https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 200159 (195K) [text/plain]\n",
            "Saving to: ‘vi_test.txt’\n",
            "\n",
            "vi_test.txt         100%[===================>] 195.47K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-03-03 14:40:22 (13.5 MB/s) - ‘vi_test.txt’ saved [200159/200159]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNS7jHlGs7rc"
      },
      "source": [
        "tagsetDict = {\"N\" : 1,\r\n",
        "              \"Np\" : 2,\r\n",
        "              \"Nc\" : 3,\r\n",
        "              \"Nu\" : 4,\r\n",
        "              \"V\" : 5,\r\n",
        "              \"A\" : 6,\r\n",
        "              \"P\" : 7,\r\n",
        "              \"L\" : 8,\r\n",
        "              \"M\" : 9,\r\n",
        "              \"R\" : 10,\r\n",
        "              \"E\" : 11,\r\n",
        "              \"C\" : 12,\r\n",
        "              \"I\" : 13,\r\n",
        "              \"T\" : 14,\r\n",
        "              \"B\" : 15,\r\n",
        "              \"Y\" : 16,\r\n",
        "              \"S\" : 17,\r\n",
        "              \"X\" : 19 }\r\n",
        "inverseTagsetDict = {tagsetDict[k]:k for k in tagsetDict}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pdWalBVs-Mu",
        "outputId": "4d2633be-53cb-4813-c405-db3fb21f126b"
      },
      "source": [
        "wordBank = defaultdict()\r\n",
        "bigramBank = defaultdict()\r\n",
        "bigramDict = defaultdict(int)\r\n",
        "\r\n",
        "bigramFreq = {x:[0]*18 for x in tagsetDict}\r\n",
        "mostCommonBigrams = defaultdict()\r\n",
        "\r\n",
        "prevParts = None\r\n",
        "\r\n",
        "f_tagged_train = io.open(\"vi_train.txt\", encoding='utf-8').readlines()\r\n",
        "f_tagged_test = io.open(\"vi_test.txt\", encoding='utf-8').readlines()\r\n",
        "\r\n",
        "train = f_tagged_train[:10000]\r\n",
        "test = f_tagged_test[:10000]\r\n",
        "print(train[:10])\r\n",
        "print(test[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Trên/E đường/N đi/V ,/, có/V một/M lần/N xe/N cô/N suýt/R rơi/V xuống/R vực/N ở/E đèo/N Ngoạn_Mục/Np ./.\\n', '\\n', 'Trong/E một/M trận/N đánh/V ác_liệt/A bên/N thành/N cổ/N Quảng_Trị/Np ,/, một/M loạt/N đạn/N pháo/N của/E kẻ_thù/N đã/R rơi/V trúng/A chỗ/N chiến_sĩ/N thông_tin/N Nguyễn_Văn_Thạc/Np .../... Hôm/N đó/P ngày/N 30/M -/- 7/M -/- 1972/M ./.\\n', '\\n', 'Sau/E khi/N H./Ny “/“ AK/Ny ”/” bị/V xộ/V khám/V ,/, Hoàng/Np đã/R hoàn_lương/V ,/, bây_giờ/P đang/R phụ/V việc/N cho/E gia_đình/N Quân/Np ./.\\n', '\\n', 'Hà/Np ,/, 21/M tuổi/N -/- làm/V nghề/N hớt_tóc/V ,/, khai/V :/: “/“ Ban_đầu/N nghe/V mấy/L ảnh/N nói/V thuốc/N lắc/V không/R nghiện/V nên/C em/N uống/V thử/V ./.\\n', '\\n', 'Theo/E anh/Nc Thông/Np ,/, người/N già/A ở/E Đức/Np thường/R sống/V một_mình/Ny và/C sống/V với/E những/L kỷ_vật/N của/E riêng/A mình/P mang/V về/R từ/E những/L chuyến/N du_lịch/V hay/C những/L tặng_vật/N của/E cả/P một/M đời/N ./.\\n', '\\n']\n",
            "['Trên/E đường/N xuất_hiện/V nhiều/A cặp/N cha/N -/- con/N ,/, mẹ/N -/- con/N ,/, hay/C có_khi/Ny là/C cả/T nhà/N ,/, tay_xách_nách_mang/Ny vừa/R đi/V vừa/R dò/V bản_đồ/N ./.\\n', '\\n', 'Tại/E vũ_trường/N N/Ny nổi_tiếng/A ,/, dân_chơi/N gọi/V rượu/N uống/V không/R hết/V thì/C gửi/V ,/, riêng/A Yến/Np .../... bỏ/V luôn/R ,/, mai/N thích/V gọi/V tiếp/V ./.\\n', '\\n', 'Xét_nghiệm/V ,/, uống/V thuốc/N ,/, phản_ứng/V thuốc/N ,/, sốt/V cao/A ,/, sụt/V cân/N ,/, căng_thẳng/A .../... nhưng/C câu_chuyện/N của/E các/L anh/N ,/, các/L chị/N kể/V về/E những/L nguy_hiểm/N sát/A cạnh/N mình/P nghe/V cứ/R nhẹ/A như_không/Ny ,/, chẳng/R hề/T có/V ý_định/N bỏ_cuộc/V ./.\\n', '\\n', 'Tại/E những/L nơi/N này/P ,/, lượng/N người/N đi/V bắt/V và/C “/“ sản_lượng/N ”/” khá/R cao/A nên/C việc/Nc thu_mua/V khá/R nhộn_nhịp/A ./.\\n', '\\n', 'Khi/N tàu/N quay/V về/E đất_liền/N ,/, được/V đưa/V vào/E bệnh_viện/N để/E mổ/V Đức/Np chỉ/R còn/V 40/M kg/Nu ,/, mất/V 20/M kg/Nu so/V với/E ngày/N bước/V lên/R tàu/N ./.\\n', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p7qjF5tkh4k"
      },
      "source": [
        "for line in train:\r\n",
        "    l_split = line.split()\r\n",
        "    for i,w in enumerate(l_split):\r\n",
        "        parts = w.split(\"/\")\r\n",
        "        if i >= 1:\r\n",
        "            prevParts = l_split[i-1].split(\"/\")\r\n",
        "        if len(parts) == 1 or \\\r\n",
        "            parts[1] not in tagsetDict:\r\n",
        "            continue\r\n",
        "\r\n",
        "        if i >= 2 and prevParts[1] in string.punctuation:\r\n",
        "            prevParts = l_split[i-1].split(\"/\")\r\n",
        "\r\n",
        "        word = parts[0]\r\n",
        "        pos = parts[1]\r\n",
        "\r\n",
        "        if i >= 1:\r\n",
        "            prevWord = prevParts[0]\r\n",
        "            prevPos = prevParts[1]\r\n",
        "            bigramBank[ word ] = (pos, prevWord, prevPos)\r\n",
        "            bigramDict[ (pos, prevPos) ] += 1\r\n",
        "\r\n",
        "        if word not in wordBank:\r\n",
        "            wordBank[word] = [pos]\r\n",
        "        else:\r\n",
        "            wordBank[word] += [pos]\r\n",
        "#print(wordBank)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsjZy2NYRMWi"
      },
      "source": [
        "bi_grams=[]\r\n",
        "tri_grams=[]\r\n",
        "qua_grams=[]\r\n",
        "for sentence in set(wordBank):\r\n",
        "  temp=0\r\n",
        "  for s in list(sentence):\r\n",
        "    if s == \"_\":\r\n",
        "      temp+=1\r\n",
        "    if temp == 0:\r\n",
        "      bi_grams.append(sentence)\r\n",
        "    elif temp == 2:\r\n",
        "      tri_grams.append(sentence)\r\n",
        "    elif temp == 4:\r\n",
        "      qua_grams.append(sentence)\r\n",
        "#print(bi_grams)\r\n",
        "#print(tri_grams)\r\n",
        "#print(qua_grams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOZ-3nDLoZDS"
      },
      "source": [
        "#finished getting training data\r\n",
        "for k in bigramFreq:\r\n",
        "    maxFreq = 0\r\n",
        "    maxPos = \"X\"\r\n",
        "    for i,x in enumerate(bigramFreq[k]):\r\n",
        "        if x > maxFreq:\r\n",
        "            maxFreq = x\r\n",
        "            maxPos = inverseTagsetDict[i]\r\n",
        "    mostCommonBigrams[k] = maxPos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqUbzfh4ocjD"
      },
      "source": [
        "def Viterbi(word,wordIdx, lineSize, line):\r\n",
        "    feat = [1]\r\n",
        "    sentPercent = float(wordIdx)/float(lineSize)\r\n",
        "    feat.append(sentPercent)\r\n",
        "\r\n",
        "    if word[0].isupper() and wordIdx != 0:\r\n",
        "        feat.append(1)\r\n",
        "    else:\r\n",
        "        feat.append(0)\r\n",
        "\r\n",
        "    posIdx_array = ([0] * len(tagsetDict))\r\n",
        "    posSet = []\r\n",
        "    if word in wordBank:\r\n",
        "        posSet = wordBank[word]\r\n",
        "    else:\r\n",
        "        if wordIdx == 0:\r\n",
        "            posSet = list(tagsetDict.keys())[0] \r\n",
        "            posIdx_array[tagsetDict[posSet]] = 1\r\n",
        "            return feat + posIdx_array + [0]\r\n",
        "        else:\r\n",
        "            prevWord = line[wordIdx-1]\r\n",
        "            if prevWord in wordBank:\r\n",
        "                prevPos = wordBank[prevWord]\r\n",
        "                maxPos = mostCommonBigrams[prevPos]\r\n",
        "                if prevPos == \"E\":\r\n",
        "                    maxPos = 3\r\n",
        "                posIdx_array[tagsetDict[maxPos]] = 1\r\n",
        "                feat += posIdx_array + [tagsetDict[maxPos]]\r\n",
        "                return feat\r\n",
        "            else:\r\n",
        "                posSet = list(tagsetDict.keys())[0]  \r\n",
        "                posIdx_array[tagsetDict[posSet]] = 1\r\n",
        "                feat += posIdx_array + [0]\r\n",
        "                return feat\r\n",
        "\r\n",
        "    for pos in posSet:\r\n",
        "        posIdx = tagsetDict[pos]\r\n",
        "        posIdx_array[posIdx] += 1.0/len(wordBank[word])\r\n",
        "    feat += (posIdx_array) + [0]\r\n",
        "    return feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY6pPkMhoiEO"
      },
      "source": [
        "#finished creating y and xtrain\r\n",
        "y = []\r\n",
        "X_train = []\r\n",
        "for line in train:\r\n",
        "    l_split = line.split()\r\n",
        "    for i,w in enumerate(l_split):\r\n",
        "        parts = w.split(\"/\")\r\n",
        "        word = parts[0]\r\n",
        "        len_line = len(l_split)\r\n",
        "\r\n",
        "        if len(parts) == 1 or parts[1] not in tagsetDict:\r\n",
        "            continue\r\n",
        "        y.append(wordBank[word][0])\r\n",
        "        X_train.append( Viterbi(word,i,len_line, l_split))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nykex7zONMF1",
        "outputId": "af80f1d8-43c2-4c6d-a226-82e366cc898d"
      },
      "source": [
        "print (len(X_train))\r\n",
        "print (len(y))\r\n",
        "print(X_train[0:10])\r\n",
        "print(y[0:10])\r\n",
        "train_fit = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95807\n",
            "95807\n",
            "[[1, 0.0, 0, 0, 0, 0, 0, 0, 0, 0.06666666666666667, 0, 0, 0, 0, 0.9333333333333332, 0, 0, 0, 0, 0, 0, 0], [1, 0.058823529411764705, 0, 0, 0.9999999999999974, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0.11764705882352941, 0, 0, 0, 0, 0, 0, 0.8996598639455661, 0, 0, 0, 0, 0.09353741496598644, 0, 0, 0, 0.006802721088435374, 0, 0, 0, 0], [1, 0.23529411764705882, 0, 0, 0, 0, 0, 0, 0.9928644240571012, 0, 0, 0, 0, 0.004077471967380225, 0, 0, 0, 0.0030581039755351687, 0, 0, 0, 0], [1, 0.29411764705882354, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0000000000000167, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0.35294117647058826, 0, 0, 0.9731182795698917, 0, 0, 0, 0.02688172043010753, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0.4117647058823529, 0, 0, 0.9900990099009908, 0, 0, 0, 0.009900990099009901, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0.47058823529411764, 0, 0, 0.7110091743119239, 0, 0.2889908256880732, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0.5294117647058824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0.5882352941176471, 0, 0, 0, 0, 0, 0, 1.0000000000000002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "['E', 'N', 'V', 'V', 'M', 'N', 'N', 'N', 'R', 'V']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kn5wNeQNYNE"
      },
      "source": [
        "#finished getting testing data\r\n",
        "X_test = []\r\n",
        "correct_results = []\r\n",
        "for line in test:\r\n",
        "    l_split = line.split()\r\n",
        "    for i,w in enumerate(l_split):\r\n",
        "        len_line = len(l_split)\r\n",
        "        parts = w.split(\"/\")\r\n",
        "        if len(parts) == 1 or parts[1] not in tagsetDict:\r\n",
        "            continue\r\n",
        "        word = parts[0]\r\n",
        "        pos = parts[1]\r\n",
        "        if word in string.punctuation or word == \":.\":\r\n",
        "            continue\r\n",
        "        X_test.append(Viterbi(word,i,len_line,l_split))\r\n",
        "        correct_results.append(pos)\r\n",
        "#print(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYsB2WPmoqkg",
        "outputId": "1fc289f5-042d-4879-9ca3-31986082f90b"
      },
      "source": [
        "predicted_results = train_fit.predict(X_test)\r\n",
        "print(\"Predicted Results:\",predicted_results[:12])\r\n",
        "print(\"Correct Results  :\",correct_results[:12])\r\n",
        "\r\n",
        "numCorrect = 0\r\n",
        "for cor,pred in zip(correct_results, predicted_results):\r\n",
        "    if cor == pred:\r\n",
        "        numCorrect += 1\r\n",
        "\r\n",
        "accuracy = 1.0*numCorrect/len(correct_results)\r\n",
        "print(\"Viterbi Algorithm Accuracy:\",accuracy*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Results: ['E' 'N' 'V' 'A' 'N' 'N' 'N' 'N' 'N' 'C' 'V' 'P']\n",
            "Correct Results  : ['E', 'N', 'V', 'A', 'N', 'N', 'N', 'N', 'N', 'C', 'C', 'T']\n",
            "Viterbi Algorithm Accuracy: 86.96898351929573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk_C19f_RZqb"
      },
      "source": [
        "import unicodedata as ud\r\n",
        "import re\r\n",
        "def syllablize(sentence):\r\n",
        "    word = '\\w+'\r\n",
        "    non_word = '[^\\w\\s]'\r\n",
        "    digits = '\\d+([\\.,_]\\d+)+'\r\n",
        "    \r\n",
        "    patterns = []\r\n",
        "    patterns.extend([word, non_word, digits])\r\n",
        "    patterns = f\"({'|'.join(patterns)})\"\r\n",
        "    \r\n",
        "    sentence = ud.normalize('NFC', sentence)\r\n",
        "    tokens = re.findall(patterns, sentence, re.UNICODE)\r\n",
        "    return [token[0] for token in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOoF0kH-QhOE"
      },
      "source": [
        "def longest_matching(sentence, bi_grams, tri_grams):\r\n",
        "    syllables = syllablize(sentence)\r\n",
        "    syl_len = len(syllables)\r\n",
        "    \r\n",
        "    curr_id = 0\r\n",
        "    word_list = []\r\n",
        "    done = False\r\n",
        "    \r\n",
        "    while (curr_id < syl_len) and (not done):\r\n",
        "        curr_word = syllables[curr_id]\r\n",
        "        if curr_id >= syl_len - 1:\r\n",
        "            word_list.append(curr_word)\r\n",
        "            done = True\r\n",
        "        else:\r\n",
        "            next_word = syllables[curr_id + 1]\r\n",
        "            pair_word = ' '.join([curr_word.lower(), next_word.lower()])\r\n",
        "            if curr_id >= (syl_len - 2):\r\n",
        "                if pair_word in bi_grams:\r\n",
        "                    word_list.append('_'.join([curr_word, next_word]))\r\n",
        "                    curr_id += 2\r\n",
        "                else:\r\n",
        "                    word_list.append(curr_word)\r\n",
        "                    curr_id += 1\r\n",
        "            else:\r\n",
        "                next_next_word = syllables[curr_id + 2]\r\n",
        "                triple_word = ' '.join([pair_word, next_next_word.lower()])\r\n",
        "                if triple_word in tri_grams:\r\n",
        "                    word_list.append('_'.join([curr_word, next_word, next_next_word]))\r\n",
        "                    curr_id += 3\r\n",
        "                elif pair_word in bi_grams:\r\n",
        "                    word_list.append('_'.join([curr_word, next_word]))\r\n",
        "                    curr_id += 2\r\n",
        "                else:\r\n",
        "                    word_list.append(curr_word)\r\n",
        "                    curr_id += 1\r\n",
        "    return word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUPlsYnEYoyQ"
      },
      "source": [
        "def toString(wl):\r\n",
        "  wl=longest_matching(wl, bi_grams, tri_grams)\r\n",
        "  X=[]\r\n",
        "  A=[]\r\n",
        "  text=\"\"\r\n",
        "  for i in set(wl):\r\n",
        "    if i ==\".\":\r\n",
        "      A=\"./.\"\r\n",
        "    else:\r\n",
        "      X.append(Viterbi(i,1,1,1))\r\n",
        "      A=str(train_fit.predict(X))\r\n",
        "  #print(A)  \r\n",
        "  for i in range(len(wl)):\r\n",
        "    text+=wl[i]\r\n",
        "    text+='/'\r\n",
        "    text+=str(A[2])\r\n",
        "    text+=' '\r\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYKdG-g7Qbjp",
        "outputId": "62043dd6-5991-4911-b39f-6a49eb582281"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "    #wr = input('Text: ')\r\n",
        "    wr =\"Dù khá đắt nhưng tôi vẫn đồng ý.\"\r\n",
        "    print(wr)\r\n",
        "    wl = ViTokenizer.tokenize(wr)\r\n",
        "    wl = wl.split()  \r\n",
        "    for i in wl:\r\n",
        "      try:\r\n",
        "          print(toString(i),end='')\r\n",
        "      except Exception:\r\n",
        "          print(i,\"X\",sep='/',end='')\r\n",
        "          print(' ',end='')\r\n",
        "          pass    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dù khá đắt nhưng tôi vẫn đồng ý.\n",
            "Dù/C khá/R đắt/A nhưng/C tôi/P vẫn/R đồng_ý/V ./. "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}