{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vietnamese_POS_Tagger.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNo54xBnYMkY3C1+8l3vmEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nthanhkhang/vietnamese-pos-tagging/blob/main/Vietnamese_POS_Tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcrL9Cxkl7pA"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyePADQUsrNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a216a44c-da8a-4916-cee5-68ed2b9097e9"
      },
      "source": [
        "!pip install pyvi\r\n",
        "\r\n",
        "import io\r\n",
        "import string\r\n",
        "import numpy\r\n",
        "import random\r\n",
        "import sklearn.svm as svm\r\n",
        "\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from collections import defaultdict\r\n",
        "from pyvi import ViTokenizer, ViPosTagger\r\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.7/dist-packages (0.1)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.3.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.22.2.post1)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHivsERgmTuj"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PguXY_QMKw4H",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5875d12f-0e42-440f-95c6-474b7b231deb"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_train.txt\n",
        "!wget https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 13:20:33--  https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1664452 (1.6M) [text/plain]\n",
            "Saving to: ‘vi_train.txt.2’\n",
            "\n",
            "\rvi_train.txt.2        0%[                    ]       0  --.-KB/s               \rvi_train.txt.2      100%[===================>]   1.59M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-03-07 13:20:33 (16.1 MB/s) - ‘vi_train.txt.2’ saved [1664452/1664452]\n",
            "\n",
            "--2021-03-07 13:20:33--  https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 200159 (195K) [text/plain]\n",
            "Saving to: ‘vi_test.txt.2’\n",
            "\n",
            "vi_test.txt.2       100%[===================>] 195.47K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-03-07 13:20:33 (6.52 MB/s) - ‘vi_test.txt.2’ saved [200159/200159]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuDgP9pYmgPx"
      },
      "source": [
        "# Tagset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNS7jHlGs7rc"
      },
      "source": [
        "tagsetDict = {\"N\" : 1,\r\n",
        "              \"Np\" : 2,\r\n",
        "              \"Nc\" : 3,\r\n",
        "              \"Nu\" : 4,\r\n",
        "              \"V\" : 5,\r\n",
        "              \"A\" : 6,\r\n",
        "              \"P\" : 7,\r\n",
        "              \"L\" : 8,\r\n",
        "              \"M\" : 9,\r\n",
        "              \"R\" : 10,\r\n",
        "              \"E\" : 11,\r\n",
        "              \"C\" : 12,\r\n",
        "              \"I\" : 13,\r\n",
        "              \"T\" : 14,\r\n",
        "              \"B\" : 15,\r\n",
        "              \"Y\" : 16,\r\n",
        "              \"S\" : 17,\r\n",
        "              \"X\" : 18 }\r\n",
        "inverseTagsetDict = {tagsetDict[k]:k for k in tagsetDict}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLM5GvuHnZSu"
      },
      "source": [
        "# Check some of the tagged words.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pdWalBVs-Mu",
        "outputId": "88890f10-6baa-4135-d816-a9dc8e391b8c"
      },
      "source": [
        "wordBank = defaultdict()\r\n",
        "bigramBank = defaultdict()\r\n",
        "bigramDict = defaultdict(int)\r\n",
        "\r\n",
        "bigramFreq = {x:[0]*18 for x in tagsetDict}\r\n",
        "mostCommonBigrams = defaultdict()\r\n",
        "\r\n",
        "prevParts = None\r\n",
        "\r\n",
        "f_tagged_train = io.open(\"vi_train.txt\", encoding='utf-8').readlines()\r\n",
        "f_tagged_test = io.open(\"vi_test.txt\", encoding='utf-8').readlines()\r\n",
        "\r\n",
        "train = f_tagged_train[:12100]\r\n",
        "test = f_tagged_test[:10000]\r\n",
        "print(train[:10])\r\n",
        "print(test[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', 'Trên/E đường/N đi/V ,/, có/V một/M lần/N xe/N cô/N suýt/R rơi/V xuống/R vực/N ở/E đèo/N Ngoạn_Mục/Np ./.\\n', '\\n', 'Trong/E một/M trận/N đánh/V ác_liệt/A bên/N thành/N cổ/N Quảng_Trị/Np ,/, một/M loạt/N đạn/N pháo/N của/E kẻ_thù/N đã/R rơi/V trúng/A chỗ/N chiến_sĩ/N thông_tin/N Nguyễn_Văn_Thạc/Np .../... Hôm/N đó/P ngày/N 30/M -/- 7/M -/- 1972/M ./.\\n', '\\n', 'Sau/E khi/N H./Ny “/“ AK/Ny ”/” bị/V xộ/V khám/V ,/, Hoàng/Np đã/R hoàn_lương/V ,/, bây_giờ/P đang/R phụ/V việc/N cho/E gia_đình/N Quân/Np ./.\\n', '\\n', 'Hà/Np ,/, 21/M tuổi/N -/- làm/V nghề/N hớt_tóc/V ,/, khai/V :/: “/“ Ban_đầu/N nghe/V mấy/L ảnh/N nói/V thuốc/N lắc/V không/R nghiện/V nên/C em/N uống/V thử/V ./.\\n', '\\n', 'Theo/E anh/Nc Thông/Np ,/, người/N già/A ở/E Đức/Np thường/R sống/V một_mình/Ny và/C sống/V với/E những/L kỷ_vật/N của/E riêng/A mình/P mang/V về/R từ/E những/L chuyến/N du_lịch/V hay/C những/L tặng_vật/N của/E cả/P một/M đời/N ./.\\n']\n",
            "['Trên/E đường/N xuất_hiện/V nhiều/A cặp/N cha/N -/- con/N ,/, mẹ/N -/- con/N ,/, hay/C có_khi/Ny là/C cả/T nhà/N ,/, tay_xách_nách_mang/Ny vừa/R đi/V vừa/R dò/V bản_đồ/N ./.\\n', '\\n', 'Tại/E vũ_trường/N N/Ny nổi_tiếng/A ,/, dân_chơi/N gọi/V rượu/N uống/V không/R hết/V thì/C gửi/V ,/, riêng/A Yến/Np .../... bỏ/V luôn/R ,/, mai/N thích/V gọi/V tiếp/V ./.\\n', '\\n', 'Xét_nghiệm/V ,/, uống/V thuốc/N ,/, phản_ứng/V thuốc/N ,/, sốt/V cao/A ,/, sụt/V cân/N ,/, căng_thẳng/A .../... nhưng/C câu_chuyện/N của/E các/L anh/N ,/, các/L chị/N kể/V về/E những/L nguy_hiểm/N sát/A cạnh/N mình/P nghe/V cứ/R nhẹ/A như_không/Ny ,/, chẳng/R hề/T có/V ý_định/N bỏ_cuộc/V ./.\\n', '\\n', 'Tại/E những/L nơi/N này/P ,/, lượng/N người/N đi/V bắt/V và/C “/“ sản_lượng/N ”/” khá/R cao/A nên/C việc/Nc thu_mua/V khá/R nhộn_nhịp/A ./.\\n', '\\n', 'Khi/N tàu/N quay/V về/E đất_liền/N ,/, được/V đưa/V vào/E bệnh_viện/N để/E mổ/V Đức/Np chỉ/R còn/V 40/M kg/Nu ,/, mất/V 20/M kg/Nu so/V với/E ngày/N bước/V lên/R tàu/N ./.\\n', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p7qjF5tkh4k"
      },
      "source": [
        "for line in train:\r\n",
        "    l_split = line.split()\r\n",
        "    for i,w in enumerate(l_split):\r\n",
        "        parts = w.split(\"/\")\r\n",
        "        if i >= 1:\r\n",
        "            prevParts = l_split[i-1].split(\"/\")\r\n",
        "        if len(parts) == 1 or \\\r\n",
        "            parts[1] not in tagsetDict:\r\n",
        "            continue\r\n",
        "\r\n",
        "        if i >= 2 and prevParts[1] in string.punctuation:\r\n",
        "            prevParts = l_split[i-1].split(\"/\")\r\n",
        "\r\n",
        "        word = parts[0]\r\n",
        "        pos = parts[1]\r\n",
        "\r\n",
        "        if i >= 1:\r\n",
        "            prevWord = prevParts[0]\r\n",
        "            prevPos = prevParts[1]\r\n",
        "            bigramBank[ word ] = (pos, prevWord, prevPos)\r\n",
        "            bigramDict[ (pos, prevPos) ] += 1\r\n",
        "\r\n",
        "        if word not in wordBank:\r\n",
        "            wordBank[word] = [pos]\r\n",
        "        else:\r\n",
        "            wordBank[word] += [pos]\r\n",
        "#print(wordBank[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsjZy2NYRMWi"
      },
      "source": [
        "bi_grams=[]\r\n",
        "tri_grams=[]\r\n",
        "qua_grams=[]\r\n",
        "for sentence in set(wordBank):\r\n",
        "  temp=0\r\n",
        "  for s in list(sentence):\r\n",
        "    if s == \"_\":\r\n",
        "      temp+=1\r\n",
        "    if temp == 0:\r\n",
        "      bi_grams.append(sentence)\r\n",
        "    elif temp == 2:\r\n",
        "      tri_grams.append(sentence)\r\n",
        "    elif temp == 4:\r\n",
        "      qua_grams.append(sentence)\r\n",
        "#print(bi_grams)\r\n",
        "#print(tri_grams)\r\n",
        "#print(qua_grams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FMifuN9muSp"
      },
      "source": [
        "# SVM Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOZ-3nDLoZDS"
      },
      "source": [
        "#finished getting training data\r\n",
        "for k in bigramFreq:\r\n",
        "    maxFreq = 0\r\n",
        "    maxPos = \"Np\"\r\n",
        "    for i,x in enumerate(bigramFreq[k]):\r\n",
        "        if x > maxFreq:\r\n",
        "            maxFreq = x\r\n",
        "            maxPos = inverseTagsetDict[i]\r\n",
        "    mostCommonBigrams[k] = maxPos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqUbzfh4ocjD"
      },
      "source": [
        "def Viterbi_rule_based(word,wordIdx, lineSize, line):\r\n",
        "    feat = [1]\r\n",
        "    sentPercent = float(wordIdx)/float(lineSize)\r\n",
        "    feat.append(sentPercent)\r\n",
        "\r\n",
        "    if word[0].isupper() and wordIdx != 0:\r\n",
        "        feat.append(1)\r\n",
        "    else:\r\n",
        "        feat.append(0)\r\n",
        "\r\n",
        "    posIdx_array = ([0] * len(tagsetDict))\r\n",
        "    posSet = []\r\n",
        "    if word in wordBank:\r\n",
        "        posSet = wordBank[word]\r\n",
        "    else:\r\n",
        "        if wordIdx == 0:\r\n",
        "            posSet = list(tagsetDict.keys())[0] \r\n",
        "            posIdx_array[tagsetDict[posSet]] = 1\r\n",
        "            return feat + posIdx_array + [0]\r\n",
        "        else:\r\n",
        "            prevWord = line[wordIdx-1]\r\n",
        "            if prevWord in wordBank:\r\n",
        "                prevPos = wordBank[prevWord]\r\n",
        "                maxPos = mostCommonBigrams[prevPos]\r\n",
        "                if prevPos == \"E\":\r\n",
        "                    maxPos = 3\r\n",
        "                posIdx_array[tagsetDict[maxPos]] = 1\r\n",
        "                feat += posIdx_array + [tagsetDict[maxPos]]\r\n",
        "                return feat\r\n",
        "            else:\r\n",
        "                posSet = list(tagsetDict.keys())[0]  \r\n",
        "                posIdx_array[tagsetDict[posSet]] = 1\r\n",
        "                feat += posIdx_array + [0]\r\n",
        "                return feat\r\n",
        "\r\n",
        "    for pos in posSet:\r\n",
        "        posIdx = tagsetDict[pos]\r\n",
        "        posIdx_array[posIdx] += 1.0/len(wordBank[word])\r\n",
        "    feat += (posIdx_array) + [0]\r\n",
        "    return feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY6pPkMhoiEO"
      },
      "source": [
        "#finished creating y and xtrain\r\n",
        "y = []\r\n",
        "X_train = []\r\n",
        "for line in train:\r\n",
        "    l_split = line.split()\r\n",
        "    for i,w in enumerate(l_split):\r\n",
        "        parts = w.split(\"/\")\r\n",
        "        word = parts[0]\r\n",
        "        len_line = len(l_split)\r\n",
        "\r\n",
        "        if len(parts) == 1 or parts[1] not in tagsetDict:\r\n",
        "            continue\r\n",
        "        y.append(wordBank[word][0])\r\n",
        "        X_train.append( Viterbi_rule_based(word,i,len_line, l_split))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nykex7zONMF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0601edf-5b09-4725-cc11-49a5b16f2981"
      },
      "source": [
        "print (len(X_train))\r\n",
        "print (len(y))\r\n",
        "for i in range(10):\r\n",
        "  print(i,\":\",X_train[i])\r\n",
        "print(y[:10])\r\n",
        "train_fit = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115615\n",
            "115615\n",
            "0 : [1, 0.0, 0, 0, 0, 0, 0, 0, 0, 0.05555555555555555, 0, 0, 0, 0, 0.9444444444444448, 0, 0, 0, 0, 0, 0, 0]\n",
            "1 : [1, 0.058823529411764705, 0, 0, 0.9999999999999969, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "2 : [1, 0.11764705882352941, 0, 0, 0, 0, 0, 0, 0.8943560057887214, 0, 0, 0, 0, 0.09985528219971068, 0, 0, 0, 0.005788712011577424, 0, 0, 0, 0]\n",
            "3 : [1, 0.23529411764705882, 0, 0, 0, 0, 0, 0, 0.9917491749175101, 0, 0, 0, 0, 0.005775577557755774, 0, 0, 0, 0.0024752475247524753, 0, 0, 0, 0]\n",
            "4 : [1, 0.29411764705882354, 0, 0, 0.0007412898443291327, 0, 0, 0, 0, 0, 0, 0, 0.9992587101556422, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "5 : [1, 0.35294117647058826, 0, 0, 0.977973568281941, 0, 0, 0, 0.022026431718061675, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "6 : [1, 0.4117647058823529, 0, 0, 0.9916666666666656, 0, 0, 0, 0.008333333333333333, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "7 : [1, 0.47058823529411764, 0, 0, 0.7067669172932364, 0, 0.2932330827067672, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "8 : [1, 0.5294117647058824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "9 : [1, 0.5882352941176471, 0, 0, 0, 0, 0, 0, 0.9999999999999996, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['E', 'N', 'V', 'V', 'M', 'N', 'N', 'N', 'R', 'V']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rx8U3KazWjY"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPpnSv_8s0Ze"
      },
      "source": [
        "# Training Text Classification Model and Predicting Sentiment\r\n",
        "#from sklearn.ensemble import RandomForestClassifier\r\n",
        "#classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\r\n",
        "#train_fit =classifier.fit(X_train, y) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kn5wNeQNYNE"
      },
      "source": [
        "#finished getting testing data\r\n",
        "X_test = []\r\n",
        "correct_results = []\r\n",
        "for line in test:\r\n",
        "    l_split = line.split()\r\n",
        "    for i,w in enumerate(l_split):\r\n",
        "        len_line = len(l_split)\r\n",
        "        parts = w.split(\"/\")\r\n",
        "        if len(parts) == 1 or parts[1] not in tagsetDict:\r\n",
        "            continue\r\n",
        "        word = parts[0]\r\n",
        "        pos = parts[1]\r\n",
        "        if word in string.punctuation or word == \":.\":\r\n",
        "            continue\r\n",
        "        X_test.append(Viterbi_rule_based(word,i,len_line,l_split))\r\n",
        "        correct_results.append(pos)\r\n",
        "#print(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0X24M9ZnyOB"
      },
      "source": [
        "# Using the Longest Matching algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk_C19f_RZqb"
      },
      "source": [
        "import unicodedata as ud\r\n",
        "import re\r\n",
        "def syllablize(sentence):\r\n",
        "    word = '\\w+'\r\n",
        "    non_word = '[^\\w\\s]'\r\n",
        "    digits = '\\d+([\\.,_]\\d+)+'\r\n",
        "    \r\n",
        "    patterns = []\r\n",
        "    patterns.extend([word, non_word, digits])\r\n",
        "    patterns = f\"({'|'.join(patterns)})\"\r\n",
        "    \r\n",
        "    sentence = ud.normalize('NFC', sentence)\r\n",
        "    tokens = re.findall(patterns, sentence, re.UNICODE)\r\n",
        "    return [token[0] for token in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOoF0kH-QhOE"
      },
      "source": [
        "def longest_matching(sentence, bi_grams, tri_grams):\r\n",
        "    syllables = syllablize(sentence)\r\n",
        "    syl_len = len(syllables)\r\n",
        "    \r\n",
        "    curr_id = 0\r\n",
        "    word_list = []\r\n",
        "    done = False\r\n",
        "    \r\n",
        "    while (curr_id < syl_len) and (not done):\r\n",
        "        curr_word = syllables[curr_id]\r\n",
        "        if curr_id >= syl_len - 1:\r\n",
        "            word_list.append(curr_word)\r\n",
        "            done = True\r\n",
        "        else:\r\n",
        "            next_word = syllables[curr_id + 1]\r\n",
        "            pair_word = ' '.join([curr_word.lower(), next_word.lower()])\r\n",
        "            if curr_id >= (syl_len - 2):\r\n",
        "                if pair_word in bi_grams:\r\n",
        "                    word_list.append('_'.join([curr_word, next_word]))\r\n",
        "                    curr_id += 2\r\n",
        "                else:\r\n",
        "                    word_list.append(curr_word)\r\n",
        "                    curr_id += 1\r\n",
        "            else:\r\n",
        "                next_next_word = syllables[curr_id + 2]\r\n",
        "                triple_word = ' '.join([pair_word, next_next_word.lower()])\r\n",
        "                if triple_word in tri_grams:\r\n",
        "                    word_list.append('_'.join([curr_word, next_word, next_next_word]))\r\n",
        "                    curr_id += 3\r\n",
        "                elif pair_word in bi_grams:\r\n",
        "                    word_list.append('_'.join([curr_word, next_word]))\r\n",
        "                    curr_id += 2\r\n",
        "                else:\r\n",
        "                    word_list.append(curr_word)\r\n",
        "                    curr_id += 1\r\n",
        "    return word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHvjWHIphS-t"
      },
      "source": [
        "# Conver into String"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUPlsYnEYoyQ"
      },
      "source": [
        "def toString(wl):\r\n",
        "  wl=longest_matching(wl, bi_grams, tri_grams)\r\n",
        "  X=[]\r\n",
        "  A=[]\r\n",
        "  text=\"\"\r\n",
        "  for i in set(wl):\r\n",
        "    if i ==\".\":\r\n",
        "      A=\"./.\"\r\n",
        "    else:\r\n",
        "      X.append(Viterbi_rule_based(i,1,1,1))\r\n",
        "      A=str(train_fit.predict(X))\r\n",
        "  #print(A)  \r\n",
        "  for i in range(len(wl)):\r\n",
        "    text+=wl[i]\r\n",
        "    text+='/'\r\n",
        "    text+=str(A[2])\r\n",
        "    text+=' '\r\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLON4bchnjbL"
      },
      "source": [
        "# Test accuracy on subset of test data\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYsB2WPmoqkg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8186bbb0-6bc0-427a-a9b9-4884fdc3b10b"
      },
      "source": [
        "predicted_results = train_fit.predict(X_test)\r\n",
        "print(\"Predicted Results:\",predicted_results[:12])\r\n",
        "print(\"Correct Results  :\",correct_results[:12])\r\n",
        "\r\n",
        "numCorrect = 0\r\n",
        "for cor,pred in zip(correct_results, predicted_results):\r\n",
        "    if cor == pred:\r\n",
        "        numCorrect += 1\r\n",
        "\r\n",
        "accuracy = 1.0*numCorrect/len(correct_results)\r\n",
        "print(\"Viterbi Algorithm Accuracy:\",accuracy*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Results: ['E' 'N' 'V' 'A' 'N' 'N' 'N' 'N' 'N' 'C' 'V' 'P']\n",
            "Correct Results  : ['E', 'N', 'V', 'A', 'N', 'N', 'N', 'N', 'N', 'C', 'C', 'T']\n",
            "Viterbi Algorithm Accuracy: 87.36308731702323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njxLzCh4qr2V",
        "outputId": "a9efc670-0871-4ba3-eac8-1033ccd46ed7"
      },
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "print('Results of the Markov hidden model combined with the Viterbi algorithm:\\n')\r\n",
        "print(classification_report(predicted_results, correct_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of the Markov hidden model combined with the Viterbi algorithm:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.77      0.90      0.83      1242\n",
            "           C       0.88      0.93      0.91       782\n",
            "           E       0.93      0.82      0.87      1641\n",
            "           I       0.33      0.75      0.46         4\n",
            "           L       0.97      0.93      0.95       432\n",
            "           M       0.90      1.00      0.94       742\n",
            "           N       0.95      0.82      0.88      6506\n",
            "          Nc       0.53      0.78      0.63       399\n",
            "          Np       0.77      0.98      0.86       696\n",
            "          Nu       0.82      0.91      0.87       102\n",
            "           P       0.98      0.95      0.96       992\n",
            "           R       0.89      0.84      0.86      1745\n",
            "           S       0.53      0.62      0.57        16\n",
            "           T       0.28      0.69      0.40        59\n",
            "           V       0.84      0.92      0.88      4179\n",
            "           Y       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.87     19538\n",
            "   macro avg       0.71      0.80      0.74     19538\n",
            "weighted avg       0.89      0.87      0.88     19538\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlUMx7Fyonf0"
      },
      "source": [
        "# Check how a sentence is tagged by the POS tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYKdG-g7Qbjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039c6c3f-a619-4b22-90e6-fc049c654d52"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "    #wr = input('Text: ')\r\n",
        "    wr =\"Dù rất đắt nhưng tôi vẫn đồng ý.\"\r\n",
        "    print(wr)\r\n",
        "    wl = ViTokenizer.tokenize(wr)\r\n",
        "    wl = wl.split()  \r\n",
        "    for i in wl:\r\n",
        "      try:\r\n",
        "          print(toString(i),end='')\r\n",
        "      except Exception:\r\n",
        "          print(i,\"X\",sep='/',end='')\r\n",
        "          print(' ',end='')\r\n",
        "          pass    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dù rất đắt nhưng tôi vẫn đồng ý.\n",
            "Dù/C rất/R đắt/A nhưng/C tôi/P vẫn/R đồng_ý/V ./. "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}