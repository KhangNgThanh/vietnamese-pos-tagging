{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "518H0372_Nguyễn Thành Khang_SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcrL9Cxkl7pA"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IyePADQUsrNf",
        "outputId": "00ec99f0-ae9c-4bd1-f7fe-62c1602369ed"
      },
      "source": [
        "!pip install pyvi\r\n",
        "\r\n",
        "import io\r\n",
        "import string\r\n",
        "import numpy\r\n",
        "import random\r\n",
        "import sklearn.svm as svm\r\n",
        "\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from collections import defaultdict\r\n",
        "from pyvi import ViTokenizer, ViPosTagger\r\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/e1/0e5bc6b5e3327b9385d6e0f1b0a7c0404f28b74eb6db59a778515b30fd9c/pyvi-0.1-py2.py3-none-any.whl (8.5MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5MB 5.4MB/s \n",
            "\u001b[?25hCollecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.7 pyvi-0.1 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHivsERgmTuj"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PguXY_QMKw4H",
        "outputId": "af6a02eb-4b46-4e58-fc41-ae92817dbc13"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_train.txt\n",
        "!wget https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_test.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-16 11:19:19--  https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1664450 (1.6M) [text/plain]\n",
            "Saving to: ‘vi_train.txt’\n",
            "\n",
            "vi_train.txt        100%[===================>]   1.59M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-03-16 11:19:19 (16.9 MB/s) - ‘vi_train.txt’ saved [1664450/1664450]\n",
            "\n",
            "--2021-03-16 11:19:19--  https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 200159 (195K) [text/plain]\n",
            "Saving to: ‘vi_test.txt’\n",
            "\n",
            "vi_test.txt         100%[===================>] 195.47K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-03-16 11:19:20 (6.38 MB/s) - ‘vi_test.txt’ saved [200159/200159]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuDgP9pYmgPx"
      },
      "source": [
        "# Tagset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNS7jHlGs7rc"
      },
      "source": [
        "tagsetDict = {\"N\" : 1,\r\n",
        "              \"Np\" : 2,\r\n",
        "              \"Nc\" : 3,\r\n",
        "              \"Nu\" : 4,\r\n",
        "              \"V\" : 5,\r\n",
        "              \"A\" : 6,\r\n",
        "              \"P\" : 7,\r\n",
        "              \"L\" : 8,\r\n",
        "              \"M\" : 9,\r\n",
        "              \"R\" : 10,\r\n",
        "              \"E\" : 11,\r\n",
        "              \"C\" : 12,\r\n",
        "              \"I\" : 13,\r\n",
        "              \"T\" : 14,\r\n",
        "              \"B\" : 15,\r\n",
        "              \"Y\" : 16,\r\n",
        "              \"S\" : 17,\r\n",
        "              \"X\" : 18 }\r\n",
        "inverseTagsetDict = {tagsetDict[k]:k for k in tagsetDict}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLM5GvuHnZSu"
      },
      "source": [
        "# Check some of the tagged words.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pdWalBVs-Mu",
        "outputId": "448fd73d-a73b-460b-9272-d91f6da24bdb"
      },
      "source": [
        "wordBank = defaultdict()\r\n",
        "bigramBank = defaultdict()\r\n",
        "bigramDict = defaultdict(int)\r\n",
        "\r\n",
        "bigramFreq = {x:[0]*18 for x in tagsetDict}\r\n",
        "mostCommonBigrams = defaultdict()\r\n",
        "\r\n",
        "prevParts = None\r\n",
        "\r\n",
        "f_tagged_train = io.open(\"vi_train.txt\", encoding='utf-8').readlines()\r\n",
        "f_tagged_test = io.open(\"vi_test.txt\", encoding='utf-8').readlines()\r\n",
        "\r\n",
        "train = f_tagged_train[:12000]\r\n",
        "test = f_tagged_test[:10000]\r\n",
        "print(train[:10])\r\n",
        "print(test[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', 'Trên/E đường/N đi/V ,/, có/V một/M lần/N xe/N cô/N suýt/R rơi/V xuống/R vực/N ở/E đèo/N Ngoạn_Mục/Np ./.\\n', '\\n', 'Trong/E một/M trận/N đánh/V ác_liệt/A bên/N thành/N cổ/N Quảng_Trị/Np ,/, một/M loạt/N đạn/N pháo/N của/E kẻ_thù/N đã/R rơi/V trúng/A chỗ/N chiến_sĩ/N thông_tin/N Nguyễn_Văn_Thạc/Np .../... Hôm/N đó/P ngày/N 30/M -/- 7/M -/- 1972/M ./.\\n', '\\n', 'Sau/E khi/N H./Ny “/“ AK/Ny ”/” bị/V xộ/V khám/V ,/, Hoàng/Np đã/R hoàn_lương/V ,/, bây_giờ/P đang/R phụ/V việc/N cho/E gia_đình/N Quân/Np ./.\\n', '\\n', 'Hà/Np ,/, 21/M tuổi/N -/- làm/V nghề/N hớt_tóc/V ,/, khai/V :/: “/“ Ban_đầu/N nghe/V mấy/L ảnh/N nói/V thuốc/N lắc/V không/R nghiện/V nên/C em/N uống/V thử/V ./.\\n', '\\n', 'Theo/E anh/Nc Thông/Np ,/, người/N già/A ở/E Đức/Np thường/R sống/V một_mình/Ny và/C sống/V với/E những/L kỷ_vật/N của/E riêng/A mình/P mang/V về/R từ/E những/L chuyến/N du_lịch/V hay/C những/L tặng_vật/N của/E cả/P một/M đời/N ./.\\n']\n",
            "['Trên/E đường/N xuất_hiện/V nhiều/A cặp/N cha/N -/- con/N ,/, mẹ/N -/- con/N ,/, hay/C có_khi/Ny là/C cả/T nhà/N ,/, tay_xách_nách_mang/Ny vừa/R đi/V vừa/R dò/V bản_đồ/N ./.\\n', '\\n', 'Tại/E vũ_trường/N N/Ny nổi_tiếng/A ,/, dân_chơi/N gọi/V rượu/N uống/V không/R hết/V thì/C gửi/V ,/, riêng/A Yến/Np .../... bỏ/V luôn/R ,/, mai/N thích/V gọi/V tiếp/V ./.\\n', '\\n', 'Xét_nghiệm/V ,/, uống/V thuốc/N ,/, phản_ứng/V thuốc/N ,/, sốt/V cao/A ,/, sụt/V cân/N ,/, căng_thẳng/A .../... nhưng/C câu_chuyện/N của/E các/L anh/N ,/, các/L chị/N kể/V về/E những/L nguy_hiểm/N sát/A cạnh/N mình/P nghe/V cứ/R nhẹ/A như_không/Ny ,/, chẳng/R hề/T có/V ý_định/N bỏ_cuộc/V ./.\\n', '\\n', 'Tại/E những/L nơi/N này/P ,/, lượng/N người/N đi/V bắt/V và/C “/“ sản_lượng/N ”/” khá/R cao/A nên/C việc/Nc thu_mua/V khá/R nhộn_nhịp/A ./.\\n', '\\n', 'Khi/N tàu/N quay/V về/E đất_liền/N ,/, được/V đưa/V vào/E bệnh_viện/N để/E mổ/V Đức/Np chỉ/R còn/V 40/M kg/Nu ,/, mất/V 20/M kg/Nu so/V với/E ngày/N bước/V lên/R tàu/N ./.\\n', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p7qjF5tkh4k"
      },
      "source": [
        "for line in train:\r\n",
        "    l_split = line.split()\r\n",
        "    #print(l_split)\r\n",
        "    for i,w in enumerate(l_split):\r\n",
        "        parts = w.split(\"/\")\r\n",
        "        if i >= 1:\r\n",
        "            prevParts = l_split[i-1].split(\"/\")\r\n",
        "        if len(parts) == 1 or \\\r\n",
        "            parts[1] not in tagsetDict:\r\n",
        "            continue\r\n",
        "\r\n",
        "        if i >= 2 and prevParts[1] in string.punctuation:\r\n",
        "            prevParts = l_split[i-1].split(\"/\")\r\n",
        "\r\n",
        "        word = parts[0]\r\n",
        "        pos = parts[1]\r\n",
        "\r\n",
        "        if i >= 1:\r\n",
        "            prevWord = prevParts[0]\r\n",
        "            prevPos = prevParts[1]\r\n",
        "            bigramBank[ word ] = (pos, prevWord, prevPos)\r\n",
        "            bigramDict[ (pos, prevPos) ] += 1\r\n",
        "\r\n",
        "        if word not in wordBank:\r\n",
        "            wordBank[word] = [pos]\r\n",
        "        else:\r\n",
        "            wordBank[word] += [pos]\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsjZy2NYRMWi"
      },
      "source": [
        "bi_grams=[]\r\n",
        "tri_grams=[]\r\n",
        "qua_grams=[]\r\n",
        "for sentence in set(wordBank):\r\n",
        "  temp=0\r\n",
        "  for s in list(sentence):\r\n",
        "    if s == \"_\":\r\n",
        "      temp+=1\r\n",
        "    if temp == 0:\r\n",
        "      bi_grams.append(sentence)\r\n",
        "    elif temp == 2:\r\n",
        "      tri_grams.append(sentence)\r\n",
        "    elif temp == 4:\r\n",
        "      qua_grams.append(sentence)\r\n",
        "#print(bi_grams)\r\n",
        "#print(tri_grams)\r\n",
        "#print(qua_grams)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOZ-3nDLoZDS"
      },
      "source": [
        "#finished getting training data\r\n",
        "for k in bigramFreq:\r\n",
        "    maxFreq = 0\r\n",
        "    maxPos = \"Np\"\r\n",
        "    for i,x in enumerate(bigramFreq[k]):\r\n",
        "        if x > maxFreq:\r\n",
        "            maxFreq = x\r\n",
        "            maxPos = inverseTagsetDict[i]\r\n",
        "    mostCommonBigrams[k] = maxPos"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqUbzfh4ocjD"
      },
      "source": [
        "def Viterbi_rule_based(word,wordIdx, lineSize, line):\r\n",
        "    feat = [1]\r\n",
        "    sentPercent = float(wordIdx)/float(lineSize)\r\n",
        "    feat.append(sentPercent)\r\n",
        "\r\n",
        "    if word[0].isupper() and wordIdx != 0:\r\n",
        "        feat.append(1)\r\n",
        "    else:\r\n",
        "        feat.append(0)\r\n",
        "\r\n",
        "    posIdx_array = ([0] * len(tagsetDict))\r\n",
        "    posSet = []\r\n",
        "    if word in wordBank:\r\n",
        "        posSet = wordBank[word]\r\n",
        "    else:\r\n",
        "        if wordIdx == 0:\r\n",
        "            posSet = list(tagsetDict.keys())[0] \r\n",
        "            posIdx_array[tagsetDict[posSet]] = 1\r\n",
        "            return feat + posIdx_array + [0]\r\n",
        "        else:\r\n",
        "            prevWord = line[wordIdx-1]\r\n",
        "            if prevWord in wordBank:\r\n",
        "                prevPos = wordBank[prevWord]\r\n",
        "                maxPos = mostCommonBigrams[prevPos]\r\n",
        "                if prevPos == \"E\":\r\n",
        "                    maxPos = 3\r\n",
        "                posIdx_array[tagsetDict[maxPos]] = 1\r\n",
        "                feat += posIdx_array + [tagsetDict[maxPos]]\r\n",
        "                return feat\r\n",
        "            else:\r\n",
        "                posSet = list(tagsetDict.keys())[0]  \r\n",
        "                posIdx_array[tagsetDict[posSet]] = 1\r\n",
        "                feat += posIdx_array + [0]\r\n",
        "                return feat\r\n",
        "\r\n",
        "    for pos in posSet:\r\n",
        "        posIdx = tagsetDict[pos]\r\n",
        "        posIdx_array[posIdx] += 1.0/len(wordBank[word])\r\n",
        "    feat += (posIdx_array) + [0]\r\n",
        "    #print(feat)\r\n",
        "    return feat\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY6pPkMhoiEO"
      },
      "source": [
        "#finished creating y and xtrain\r\n",
        "y = []\r\n",
        "X_train = []\r\n",
        "for line in train:\r\n",
        "    l_split = line.split()\r\n",
        "    for i,w in enumerate(l_split):\r\n",
        "        parts = w.split(\"/\")\r\n",
        "        word = parts[0]\r\n",
        "        len_line = len(l_split)\r\n",
        "\r\n",
        "        if len(parts) == 1 or parts[1] not in tagsetDict:\r\n",
        "            continue\r\n",
        "        y.append(wordBank[word][0])\r\n",
        "        X_train.append(Viterbi_rule_based(word,i,len_line, l_split))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-QvaL-YeoXN"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWn3AFkY_XfX"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nykex7zONMF1",
        "outputId": "d232aca5-e231-4bc5-80a9-b5388c8fa366"
      },
      "source": [
        "print (len(X_train))\r\n",
        "print (len(y))\r\n",
        "for i in range(10):\r\n",
        "  print(i,\":\",X_train[i])\r\n",
        "print(y[:10])\r\n",
        "train_fit = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train,y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "114577\n",
            "114577\n",
            "0 : [1, 0.0, 0, 0, 0, 0, 0, 0, 0, 0.05555555555555555, 0, 0, 0, 0, 0.9444444444444448, 0, 0, 0, 0, 0, 0, 0]\n",
            "1 : [1, 0.058823529411764705, 0, 0, 0.9999999999999959, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "2 : [1, 0.11764705882352941, 0, 0, 0, 0, 0, 0, 0.8944281524926679, 0, 0, 0, 0, 0.09970674486803513, 0, 0, 0, 0.005865102639296188, 0, 0, 0, 0]\n",
            "3 : [1, 0.23529411764705882, 0, 0, 0, 0, 0, 0, 0.9916247906197839, 0, 0, 0, 0, 0.005862646566164153, 0, 0, 0, 0.002512562814070352, 0, 0, 0, 0]\n",
            "4 : [1, 0.29411764705882354, 0, 0, 0.0007524454477050414, 0, 0, 0, 0, 0, 0, 0, 0.9992475545522823, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "5 : [1, 0.35294117647058826, 0, 0, 0.9775784753363252, 0, 0, 0, 0.02242152466367713, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "6 : [1, 0.4117647058823529, 0, 0, 0.9913793103448257, 0, 0, 0, 0.008620689655172414, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "7 : [1, 0.47058823529411764, 0, 0, 0.7094339622641509, 0, 0.29056603773584905, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "8 : [1, 0.5294117647058824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "9 : [1, 0.5882352941176471, 0, 0, 0, 0, 0, 0, 0.9999999999999996, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['E', 'N', 'V', 'V', 'M', 'N', 'N', 'N', 'R', 'V']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kn5wNeQNYNE"
      },
      "source": [
        "#finished getting testing data\r\n",
        "X_test = []\r\n",
        "correct_results = []\r\n",
        "for line in test:\r\n",
        "    l_split = line.split()\r\n",
        "    for i,w in enumerate(l_split):\r\n",
        "        len_line = len(l_split)\r\n",
        "        parts = w.split(\"/\")\r\n",
        "        if len(parts) == 1 or parts[1] not in tagsetDict:\r\n",
        "            continue\r\n",
        "        word = parts[0]\r\n",
        "        pos = parts[1]\r\n",
        "        if word in string.punctuation or word == \":.\":\r\n",
        "            continue\r\n",
        "        X_test.append(Viterbi_rule_based(word,i,len_line,l_split))\r\n",
        "        correct_results.append(pos)\r\n",
        "#print(X_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0X24M9ZnyOB"
      },
      "source": [
        "# Using the Longest Matching algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk_C19f_RZqb"
      },
      "source": [
        "import unicodedata as ud\r\n",
        "import re\r\n",
        "def syllablize(sentence):\r\n",
        "    word = '\\w+'\r\n",
        "    non_word = '[^\\w\\s]'\r\n",
        "    digits = '\\d+([\\.,_]\\d+)+'\r\n",
        "    \r\n",
        "    patterns = []\r\n",
        "    patterns.extend([word, non_word, digits])\r\n",
        "    patterns = f\"({'|'.join(patterns)})\"\r\n",
        "    \r\n",
        "    sentence = ud.normalize('NFC', sentence)\r\n",
        "    tokens = re.findall(patterns, sentence, re.UNICODE)\r\n",
        "    return [token[0] for token in tokens]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOoF0kH-QhOE"
      },
      "source": [
        "def longest_matching(sentence, bi_grams, tri_grams):\r\n",
        "    syllables = syllablize(sentence)\r\n",
        "    syl_len = len(syllables)\r\n",
        "    \r\n",
        "    curr_id = 0\r\n",
        "    word_list = []\r\n",
        "    done = False\r\n",
        "    \r\n",
        "    while (curr_id < syl_len) and (not done):\r\n",
        "        curr_word = syllables[curr_id]\r\n",
        "        if curr_id >= syl_len - 1:\r\n",
        "            word_list.append(curr_word)\r\n",
        "            done = True\r\n",
        "        else:\r\n",
        "            next_word = syllables[curr_id + 1]\r\n",
        "            pair_word = ' '.join([curr_word.lower(), next_word.lower()])\r\n",
        "            if curr_id >= (syl_len - 2):\r\n",
        "                if pair_word in bi_grams:\r\n",
        "                    word_list.append('_'.join([curr_word, next_word]))\r\n",
        "                    curr_id += 2\r\n",
        "                else:\r\n",
        "                    word_list.append(curr_word)\r\n",
        "                    curr_id += 1\r\n",
        "            else:\r\n",
        "                next_next_word = syllables[curr_id + 2]\r\n",
        "                triple_word = ' '.join([pair_word, next_next_word.lower()])\r\n",
        "                if triple_word in tri_grams:\r\n",
        "                    word_list.append('_'.join([curr_word, next_word, next_next_word]))\r\n",
        "                    curr_id += 3\r\n",
        "                elif pair_word in bi_grams:\r\n",
        "                    word_list.append('_'.join([curr_word, next_word]))\r\n",
        "                    curr_id += 2\r\n",
        "                else:\r\n",
        "                    word_list.append(curr_word)\r\n",
        "                    curr_id += 1\r\n",
        "    return word_list"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLON4bchnjbL"
      },
      "source": [
        "# Test accuracy on subset of test data\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYsB2WPmoqkg",
        "outputId": "d41ee17a-1c46-4cd2-dc22-a62857574c91"
      },
      "source": [
        "predicted_results = train_fit.predict(X_test)\r\n",
        "print(\"Predicted Results:\",predicted_results[:12])\r\n",
        "print(\"Correct Results  :\",correct_results[:12])\r\n",
        "\r\n",
        "numCorrect = 0\r\n",
        "for cor,pred in zip(correct_results, predicted_results):\r\n",
        "    if cor == pred:\r\n",
        "        numCorrect += 1\r\n",
        "\r\n",
        "accuracy = 1.0*numCorrect/len(correct_results)\r\n",
        "print(\"Viterbi Algorithm Accuracy:\",accuracy*100)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Results: ['E' 'N' 'V' 'A' 'N' 'N' 'N' 'N' 'N' 'C' 'V' 'P']\n",
            "Correct Results  : ['E', 'N', 'V', 'A', 'N', 'N', 'N', 'N', 'N', 'C', 'C', 'T']\n",
            "Viterbi Algorithm Accuracy: 87.34261439246596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njxLzCh4qr2V",
        "outputId": "bd1217a5-4eb2-4f92-847a-5fbaece0f140"
      },
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "print('Results of the Markov hidden model combined with the Viterbi algorithm:\\n')\r\n",
        "print(classification_report(predicted_results, correct_results))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of the Markov hidden model combined with the Viterbi algorithm:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.77      0.90      0.83      1241\n",
            "           C       0.88      0.93      0.91       782\n",
            "           E       0.93      0.82      0.87      1641\n",
            "           I       0.33      0.75      0.46         4\n",
            "           L       0.97      0.93      0.95       432\n",
            "           M       0.90      1.00      0.94       744\n",
            "           N       0.95      0.82      0.88      6504\n",
            "          Nc       0.53      0.78      0.63       399\n",
            "          Np       0.77      0.98      0.86       698\n",
            "          Nu       0.82      0.91      0.87       102\n",
            "           P       0.98      0.95      0.96       992\n",
            "           R       0.89      0.84      0.86      1745\n",
            "           S       0.53      0.62      0.57        16\n",
            "           T       0.28      0.69      0.40        59\n",
            "           V       0.84      0.92      0.88      4178\n",
            "           Y       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.87     19538\n",
            "   macro avg       0.71      0.80      0.74     19538\n",
            "weighted avg       0.89      0.87      0.88     19538\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlUMx7Fyonf0"
      },
      "source": [
        "# Check how a sentence is tagged by the POS tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oob99wkXUL4x",
        "outputId": "6581a861-7ad4-43b0-9c63-dd5e48c459ed"
      },
      "source": [
        "wr =\"Dù rất đắt nhưng tôi vẫn đồng ý\"\r\n",
        "A=[]\r\n",
        "X=[]\r\n",
        "wl = ViTokenizer.tokenize(wr)\r\n",
        "wl = wl.split()\r\n",
        "print(wl)\r\n",
        "#print(Viterbi_rule_based(word,i,len_line, l_split))\r\n",
        "for word in wl:\r\n",
        "  X.append(Viterbi_rule_based(word,1,len(wl),wl))\r\n",
        "X=str(train_fit.predict(X))\r\n",
        "print(X)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Dù', 'rất', 'đắt', 'nhưng', 'tôi', 'vẫn', 'đồng_ý']\n",
            "['C' 'R' 'A' 'C' 'P' 'R' 'V']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}