{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nthanhkhang/vietnamese-pos-tagging/blob/main/Vietnamese_POS_Tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyePADQUsrNf",
    "outputId": "fba7aee0-318a-4d4a-a3c9-208420527d8f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyvi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d5976ecd9a68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyvi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mViTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mViPosTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyvi'"
     ]
    }
   ],
   "source": [
    "!pip install pyvi\n",
    "import string\n",
    "import numpy\n",
    "import sklearn.svm as svm\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import io\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from pyvi import ViTokenizer, ViPosTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PguXY_QMKw4H",
    "outputId": "e7166e48-27be-4360-b839-a03daa582c92"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_train.txt\n",
    "!wget https://raw.githubusercontent.com/nthanhkhang/Natural-Language-Processing/main/Data/vi_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNS7jHlGs7rc"
   },
   "outputs": [],
   "source": [
    "tagsetDict = {\"N\" : 1,\n",
    "              \"Np\" : 2,\n",
    "              \"Nc\" : 3,\n",
    "              \"Nu\" : 4,\n",
    "              \"V\" : 5,\n",
    "              \"A\" : 6,\n",
    "              \"P\" : 7,\n",
    "              \"L\" : 8,\n",
    "              \"M\" : 9,\n",
    "              \"R\" : 10,\n",
    "              \"E\" : 11,\n",
    "              \"C\" : 12,\n",
    "              \"I\" : 13,\n",
    "              \"T\" : 14,\n",
    "              \"B\" : 15,\n",
    "              \"Y\" : 16,\n",
    "              \"S\" : 17,\n",
    "              \"X\" : 19 }\n",
    "inverseTagsetDict = {tagsetDict[k]:k for k in tagsetDict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pdWalBVs-Mu",
    "outputId": "4d2633be-53cb-4813-c405-db3fb21f126b"
   },
   "outputs": [],
   "source": [
    "wordBank = defaultdict()\n",
    "bigramBank = defaultdict()\n",
    "bigramDict = defaultdict(int)\n",
    "\n",
    "bigramFreq = {x:[0]*18 for x in tagsetDict}\n",
    "mostCommonBigrams = defaultdict()\n",
    "\n",
    "prevParts = None\n",
    "\n",
    "f_tagged_train = io.open(\"vi_train.txt\", encoding='utf-8').readlines()\n",
    "f_tagged_test = io.open(\"vi_test.txt\", encoding='utf-8').readlines()\n",
    "\n",
    "train = f_tagged_train[:10000]\n",
    "test = f_tagged_test[:10000]\n",
    "print(train[:10])\n",
    "print(test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8p7qjF5tkh4k"
   },
   "outputs": [],
   "source": [
    "for line in train:\n",
    "    l_split = line.split()\n",
    "    for i,w in enumerate(l_split):\n",
    "        parts = w.split(\"/\")\n",
    "        if i >= 1:\n",
    "            prevParts = l_split[i-1].split(\"/\")\n",
    "        if len(parts) == 1 or \\\n",
    "            parts[1] not in tagsetDict:\n",
    "            continue\n",
    "\n",
    "        if i >= 2 and prevParts[1] in string.punctuation:\n",
    "            prevParts = l_split[i-1].split(\"/\")\n",
    "\n",
    "        word = parts[0]\n",
    "        pos = parts[1]\n",
    "\n",
    "        if i >= 1:\n",
    "            prevWord = prevParts[0]\n",
    "            prevPos = prevParts[1]\n",
    "            bigramBank[ word ] = (pos, prevWord, prevPos)\n",
    "            bigramDict[ (pos, prevPos) ] += 1\n",
    "\n",
    "        if word not in wordBank:\n",
    "            wordBank[word] = [pos]\n",
    "        else:\n",
    "            wordBank[word] += [pos]\n",
    "#print(wordBank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsjZy2NYRMWi"
   },
   "outputs": [],
   "source": [
    "bi_grams=[]\n",
    "tri_grams=[]\n",
    "qua_grams=[]\n",
    "for sentence in set(wordBank):\n",
    "  temp=0\n",
    "  for s in list(sentence):\n",
    "    if s == \"_\":\n",
    "      temp+=1\n",
    "    if temp == 0:\n",
    "      bi_grams.append(sentence)\n",
    "    elif temp == 2:\n",
    "      tri_grams.append(sentence)\n",
    "    elif temp == 4:\n",
    "      qua_grams.append(sentence)\n",
    "#print(bi_grams)\n",
    "#print(tri_grams)\n",
    "#print(qua_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOZ-3nDLoZDS"
   },
   "outputs": [],
   "source": [
    "#finished getting training data\n",
    "for k in bigramFreq:\n",
    "    maxFreq = 0\n",
    "    maxPos = \"X\"\n",
    "    for i,x in enumerate(bigramFreq[k]):\n",
    "        if x > maxFreq:\n",
    "            maxFreq = x\n",
    "            maxPos = inverseTagsetDict[i]\n",
    "    mostCommonBigrams[k] = maxPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqUbzfh4ocjD"
   },
   "outputs": [],
   "source": [
    "def Viterbi(word,wordIdx, lineSize, line):\n",
    "    feat = [1]\n",
    "    sentPercent = float(wordIdx)/float(lineSize)\n",
    "    feat.append(sentPercent)\n",
    "\n",
    "    if word[0].isupper() and wordIdx != 0:\n",
    "        feat.append(1)\n",
    "    else:\n",
    "        feat.append(0)\n",
    "\n",
    "    posIdx_array = ([0] * len(tagsetDict))\n",
    "    posSet = []\n",
    "    if word in wordBank:\n",
    "        posSet = wordBank[word]\n",
    "    else:\n",
    "        if wordIdx == 0:\n",
    "            posSet = list(tagsetDict.keys())[0] \n",
    "            posIdx_array[tagsetDict[posSet]] = 1\n",
    "            return feat + posIdx_array + [0]\n",
    "        else:\n",
    "            prevWord = line[wordIdx-1]\n",
    "            if prevWord in wordBank:\n",
    "                prevPos = wordBank[prevWord]\n",
    "                maxPos = mostCommonBigrams[prevPos]\n",
    "                if prevPos == \"E\":\n",
    "                    maxPos = 3\n",
    "                posIdx_array[tagsetDict[maxPos]] = 1\n",
    "                feat += posIdx_array + [tagsetDict[maxPos]]\n",
    "                return feat\n",
    "            else:\n",
    "                posSet = list(tagsetDict.keys())[0]  \n",
    "                posIdx_array[tagsetDict[posSet]] = 1\n",
    "                feat += posIdx_array + [0]\n",
    "                return feat\n",
    "\n",
    "    for pos in posSet:\n",
    "        posIdx = tagsetDict[pos]\n",
    "        posIdx_array[posIdx] += 1.0/len(wordBank[word])\n",
    "    feat += (posIdx_array) + [0]\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DY6pPkMhoiEO"
   },
   "outputs": [],
   "source": [
    "#finished creating y and xtrain\n",
    "y = []\n",
    "X_train = []\n",
    "for line in train:\n",
    "    l_split = line.split()\n",
    "    for i,w in enumerate(l_split):\n",
    "        parts = w.split(\"/\")\n",
    "        word = parts[0]\n",
    "        len_line = len(l_split)\n",
    "\n",
    "        if len(parts) == 1 or parts[1] not in tagsetDict:\n",
    "            continue\n",
    "        y.append(wordBank[word][0])\n",
    "        X_train.append( Viterbi(word,i,len_line, l_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nykex7zONMF1",
    "outputId": "af80f1d8-43c2-4c6d-a226-82e366cc898d"
   },
   "outputs": [],
   "source": [
    "print (len(X_train))\n",
    "print (len(y))\n",
    "print(X_train[0:10])\n",
    "print(y[0:10])\n",
    "train_fit = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kn5wNeQNYNE"
   },
   "outputs": [],
   "source": [
    "#finished getting testing data\n",
    "X_test = []\n",
    "correct_results = []\n",
    "for line in test:\n",
    "    l_split = line.split()\n",
    "    for i,w in enumerate(l_split):\n",
    "        len_line = len(l_split)\n",
    "        parts = w.split(\"/\")\n",
    "        if len(parts) == 1 or parts[1] not in tagsetDict:\n",
    "            continue\n",
    "        word = parts[0]\n",
    "        pos = parts[1]\n",
    "        if word in string.punctuation or word == \":.\":\n",
    "            continue\n",
    "        X_test.append(Viterbi(word,i,len_line,l_split))\n",
    "        correct_results.append(pos)\n",
    "#print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYsB2WPmoqkg",
    "outputId": "1fc289f5-042d-4879-9ca3-31986082f90b"
   },
   "outputs": [],
   "source": [
    "predicted_results = train_fit.predict(X_test)\n",
    "print(\"Predicted Results:\",predicted_results[:12])\n",
    "print(\"Correct Results  :\",correct_results[:12])\n",
    "\n",
    "numCorrect = 0\n",
    "for cor,pred in zip(correct_results, predicted_results):\n",
    "    if cor == pred:\n",
    "        numCorrect += 1\n",
    "\n",
    "accuracy = 1.0*numCorrect/len(correct_results)\n",
    "print(\"Viterbi Algorithm Accuracy:\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uk_C19f_RZqb"
   },
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "import re\n",
    "def syllablize(sentence):\n",
    "    word = '\\w+'\n",
    "    non_word = '[^\\w\\s]'\n",
    "    digits = '\\d+([\\.,_]\\d+)+'\n",
    "    \n",
    "    patterns = []\n",
    "    patterns.extend([word, non_word, digits])\n",
    "    patterns = f\"({'|'.join(patterns)})\"\n",
    "    \n",
    "    sentence = ud.normalize('NFC', sentence)\n",
    "    tokens = re.findall(patterns, sentence, re.UNICODE)\n",
    "    return [token[0] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOoF0kH-QhOE"
   },
   "outputs": [],
   "source": [
    "def longest_matching(sentence, bi_grams, tri_grams):\n",
    "    syllables = syllablize(sentence)\n",
    "    syl_len = len(syllables)\n",
    "    \n",
    "    curr_id = 0\n",
    "    word_list = []\n",
    "    done = False\n",
    "    \n",
    "    while (curr_id < syl_len) and (not done):\n",
    "        curr_word = syllables[curr_id]\n",
    "        if curr_id >= syl_len - 1:\n",
    "            word_list.append(curr_word)\n",
    "            done = True\n",
    "        else:\n",
    "            next_word = syllables[curr_id + 1]\n",
    "            pair_word = ' '.join([curr_word.lower(), next_word.lower()])\n",
    "            if curr_id >= (syl_len - 2):\n",
    "                if pair_word in bi_grams:\n",
    "                    word_list.append('_'.join([curr_word, next_word]))\n",
    "                    curr_id += 2\n",
    "                else:\n",
    "                    word_list.append(curr_word)\n",
    "                    curr_id += 1\n",
    "            else:\n",
    "                next_next_word = syllables[curr_id + 2]\n",
    "                triple_word = ' '.join([pair_word, next_next_word.lower()])\n",
    "                if triple_word in tri_grams:\n",
    "                    word_list.append('_'.join([curr_word, next_word, next_next_word]))\n",
    "                    curr_id += 3\n",
    "                elif pair_word in bi_grams:\n",
    "                    word_list.append('_'.join([curr_word, next_word]))\n",
    "                    curr_id += 2\n",
    "                else:\n",
    "                    word_list.append(curr_word)\n",
    "                    curr_id += 1\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUPlsYnEYoyQ"
   },
   "outputs": [],
   "source": [
    "def toString(wl):\n",
    "  wl=longest_matching(wl, bi_grams, tri_grams)\n",
    "  X=[]\n",
    "  A=[]\n",
    "  text=\"\"\n",
    "  for i in set(wl):\n",
    "    if i ==\".\":\n",
    "      A=\"./.\"\n",
    "    else:\n",
    "      X.append(Viterbi(i,1,1,1))\n",
    "      A=str(train_fit.predict(X))\n",
    "  #print(A)  \n",
    "  for i in range(len(wl)):\n",
    "    text+=wl[i]\n",
    "    text+='/'\n",
    "    text+=str(A[2])\n",
    "    text+=' '\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYKdG-g7Qbjp",
    "outputId": "62043dd6-5991-4911-b39f-6a49eb582281"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #wr = input('Text: ')\n",
    "    wr =\"Dù khá đắt nhưng tôi vẫn đồng ý.\"\n",
    "    print(wr)\n",
    "    wl = ViTokenizer.tokenize(wr)\n",
    "    wl = wl.split()  \n",
    "    for i in wl:\n",
    "      try:\n",
    "          print(toString(i),end='')\n",
    "      except Exception:\n",
    "          print(i,\"X\",sep='/',end='')\n",
    "          print(' ',end='')\n",
    "          pass    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPvhzVmGjvB6ZqR4IOaRJdR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Vietnamese_POS_Tagger.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
